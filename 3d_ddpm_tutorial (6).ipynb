{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286986e",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Model on 3D data\n",
    "\n",
    "This tutorial illustrates how to use MONAI for training a denoising diffusion probabilistic model (DDPM)[1] to create synthetic 3D images.\n",
    "\n",
    "[1] - [Ho et al. \"Denoising Diffusion Probabilistic Models\"](https://arxiv.org/abs/2006.11239)\n",
    "\n",
    "\n",
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc01d24",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    CenterSpatialCropd,\n",
    "    Compose,\n",
    "    Lambdad,\n",
    "    LoadImaged,\n",
    "    Resized,\n",
    "    ScaleIntensityd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37a43",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the MONAI_DATA_DIRECTORY environment variable.\n",
    "\n",
    "This allows you to save results and reuse downloads.\n",
    "\n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b4c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"DATASET\"\n",
    "assert os.path.isdir(root_dir), \"Check PI-CAI path\"\n",
    "print(root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af1391",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8c601",
   "metadata": {},
   "source": [
    "## Setup Decathlon Dataset and training and validation data loaders\n",
    "\n",
    "In this tutorial, we will use the 3D T1 weighted brain images from the [2016 and 2017 Brain Tumor Segmentation (BraTS) challenges](https://www.med.upenn.edu/sbia/brats2017/data.html). This dataset can be easily downloaded using the [DecathlonDataset](https://docs.monai.io/en/stable/apps.html#monai.apps.DecathlonDataset) from MONAI (`task=\"Task01_BrainTumour\"`). To load the training and validation images, we are using the `data_transform` transformations that are responsible for the following:\n",
    "\n",
    "1. `LoadImaged`:  Loads the brain images from files.\n",
    "2. `Lambdad`: Choose channel 1 of the image, which is the T1-weighted image.\n",
    "3. `EnsureChannelFirstd`: Add the channel dimension of the input data.\n",
    "4. `ScaleIntensityd`: Apply a min-max scaling in the intensity values of each image to be in the `[0, 1]` range.\n",
    "5. `CenterSpatialCropd`: Crop the background of the images using a roi of size `[160, 200, 155]`.\n",
    "6. `Resized`: Resize the images to a volume with size `[32, 40, 32]`.\n",
    "\n",
    "For the data loader, we are using mini-batches of 8 images, which consumes about 21GB of GPU memory during training. Please, reduce this value to run on smaller GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = sorted(glob.glob(f\"{root_dir}/*/*_t2w.mha\"))\n",
    "print(f\"Found {len(image_paths)} volumes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd61e60",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "from monai.transforms import (\n",
    "    Transform,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    CropForegroundd,\n",
    "    RandSpatialCropd,\n",
    "    ToTensord,\n",
    "    LoadImaged,\n",
    "    ScaleIntensityRangePercentilesd,\n",
    "    EnsureChannelFirstd,\n",
    "    SpatialPadd,\n",
    "    EnsureTyped,\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "def filter_valid_3d(paths, min_depth=3):\n",
    "    \"\"\"\n",
    "    Returns only those .mha files that load as true 3-D volumes.\n",
    "    A file is dropped if:\n",
    "      • img.GetDimension() < 3   –> actually 2-D\n",
    "      • min(img.GetSize()) < min_depth –> depth too small\n",
    "    \"\"\"\n",
    "    keep, drop = [], []\n",
    "    for p in paths:\n",
    "        img = sitk.ReadImage(str(p))\n",
    "        dim = img.GetDimension()\n",
    "        size = img.GetSize()\n",
    "        if dim == 3 and min(size) >= min_depth:\n",
    "            keep.append(p)\n",
    "        else:\n",
    "            drop.append((p, dim, size))\n",
    "    print(f\"kept {len(keep)} volumes and dropped {len(drop)} bad files\")\n",
    "    if drop:\n",
    "        print(\"first few dropped:\", *(Path(d[0]).name for d in drop[:5]))\n",
    "    return keep\n",
    "    \n",
    "root_dir    = \"DATASET\"  \n",
    "all_paths = sorted(Path(root_dir).glob(\"*/*_t2w.mha\"))\n",
    "good_paths = filter_valid_3d(all_paths)\n",
    "\n",
    "image_paths = sorted(glob.glob(f\"{root_dir}/*/*_t2w.mha\"))\n",
    "train_paths, val_paths = train_test_split(\n",
    "    image_paths, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "train_dicts = [{\"image\": p} for p in train_paths]\n",
    "val_dicts   = [{\"image\": p} for p in val_paths]\n",
    "\n",
    "pixdim     = (0.5, 0.5, 1.0)\n",
    "margin     = (8, 32, 32)       \n",
    "patch_size = (64, 128, 128)    \n",
    "\n",
    "class LogIntensityStats(Transform):\n",
    "    def __init__(self, lower: float, upper: float):\n",
    "        self.lower_pct = lower\n",
    "        self.upper_pct = upper\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img = data[\"image\"].numpy().flatten()\n",
    "        low_val  = np.percentile(img, self.lower_pct * 100)\n",
    "        high_val = np.percentile(img, self.upper_pct * 100)\n",
    "        frac_low  = (img < low_val).mean()\n",
    "        frac_high = (img > high_val).mean()\n",
    "        new_min, new_max = img.min(), img.max()\n",
    "\n",
    "        logging.info(\n",
    "            f\"[IntensityStats] pct window {self.lower_pct*100:.1f}-{self.upper_pct*100:.1f} → \"\n",
    "            f\"[{low_val:.3f},{high_val:.3f}], clipped voxels: \"\n",
    "            f\"{frac_low*100:.1f}% low, {frac_high*100:.1f}% high, \"\n",
    "            f\"post-scaled range [{new_min:.3f},{new_max:.3f}]\"\n",
    "        )\n",
    "        return data\n",
    "\n",
    "transforms = Compose([\n",
    "    LoadImaged(keys=\"image\", image_only=False, reader=\"ITKReader\"),\n",
    "    EnsureTyped(keys=\"image\", track_meta=True),\n",
    "    EnsureChannelFirstd(keys=\"image\"),\n",
    "    Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
    "    Spacingd(keys=\"image\", pixdim=pixdim, mode=\"bilinear\"),\n",
    "    CropForegroundd(\n",
    "        keys=\"image\", source_key=\"image\",\n",
    "        margin=margin, select_fn=lambda x: x > 0, k_divisible=8\n",
    "    ),\n",
    "    ScaleIntensityRangePercentilesd(\n",
    "        keys=\"image\", lower=0.5, upper=99.5,\n",
    "        b_min=-1.0, b_max=1.0, clip=True\n",
    "    ),\n",
    "    LogIntensityStats(lower=0.005, upper=0.995),\n",
    "    SpatialPadd(keys=\"image\", spatial_size=patch_size, mode=\"edge\"),\n",
    "    RandSpatialCropd(keys=\"image\", roi_size=patch_size, random_size=False),\n",
    "    ToTensord(keys=\"image\"),\n",
    "])\n",
    "\n",
    "def assert_pipeline_ok(ds, atol_rot=0.1):  # Increased tolerance to 0.1\n",
    "    \"\"\"\n",
    "    Validates the preprocessing pipeline output.\n",
    "    \n",
    "    Args:\n",
    "        ds: Dataset to test\n",
    "        atol_rot: Rotation tolerance. Medical images often have small residual\n",
    "                 rotations after resampling, so 0.1 is more realistic than 1e-4\n",
    "    \"\"\"\n",
    "    s = ds[0]\n",
    "    aff = torch.as_tensor(s[\"image_meta_dict\"][\"affine\"])\n",
    "    A = aff[:3, :3]\n",
    "    \n",
    "    # Check voxel sizes\n",
    "    vs = torch.linalg.norm(A, dim=0)\n",
    "    expected_vs = torch.tensor(pixdim)\n",
    "    vs_error = (vs - expected_vs).abs().max().item()\n",
    "    print(f\"Voxel sizes: {[f'{v:.3f}' for v in vs]} (expected: {[f'{v:.3f}' for v in expected_vs]})\")\n",
    "    print(f\"Max voxel size error: {vs_error:.6f}\")\n",
    "    \n",
    "    # Check rotation\n",
    "    R = A / vs\n",
    "    rot_err = (R.abs() - torch.eye(3)).abs().max().item()\n",
    "    print(f\"Max rotation error: {rot_err:.6f}\")\n",
    "    \n",
    "    # Shape and intensity checks\n",
    "    img = s[\"image\"]\n",
    "    print(f\"Patch shape: {tuple(img.shape)}\")\n",
    "    print(f\"Intensity range: [{img.min():.3f}, {img.max():.3f}]\")\n",
    "    \n",
    "    # Assertions with helpful error messages\n",
    "    if vs_error > 0.1:  # Allow 10% voxel size error\n",
    "        print(f\" Warning: Voxel size error {vs_error:.3f} > 0.1\")\n",
    "    \n",
    "    if rot_err > atol_rot:\n",
    "        print(f\" Warning: Rotation error {rot_err:.3f} > {atol_rot}\")\n",
    "    else:\n",
    "        print(\"Orientation within tolerance\")\n",
    "    \n",
    "    # Intensity range check\n",
    "    assert -1.1 <= img.min() <= -0.8, f\"Min intensity {img.min()} not in expected range\"\n",
    "    assert 0.8 <= img.max() <= 1.1, f\"Max intensity {img.max()} not in expected range\"\n",
    "    print(\"Intensity range OK\")\n",
    "    \n",
    "    # Shape check\n",
    "    assert tuple(img.shape) == (1, *patch_size), f\"Shape {tuple(img.shape)} != expected {(1, *patch_size)}\"\n",
    "    print(\"Shape OK\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets with fixed transforms...\")\n",
    "train_ds = Dataset(data=train_dicts, transform=transforms)\n",
    "val_ds = Dataset(data=val_dicts, transform=transforms)\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"\\nTesting pipeline...\")\n",
    "assert_pipeline_ok(train_ds)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=1, shuffle=True,\n",
    "    num_workers=4, pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False,\n",
    "    num_workers=4, pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "print(\"\\n Pipeline setup complete!\")\n",
    "print(f\"Training samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(val_ds)}\")\n",
    "\n",
    "print(\"\\nChecking first 3 samples...\")\n",
    "for i in range(min(3, len(train_ds))):\n",
    "    try:\n",
    "        sample = train_ds[i]\n",
    "        img = sample[\"image\"]\n",
    "        aff = sample[\"image_meta_dict\"][\"affine\"]\n",
    "        \n",
    "        print(f\"Sample {i}: shape={tuple(img.shape)}, \"\n",
    "              f\"range=[{img.min():.2f}, {img.max():.2f}], \"\n",
    "              f\"spacing={torch.linalg.norm(aff[:3,:3], dim=0).numpy()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Sample {i}: ERROR - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab331213-7288-4947-b2c3-2080df49df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class Small3DVAE(nn.Module):\n",
    "    def __init__(self, in_ch=1, latent_ch=32, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, 32, 4, 2, 1), nn.InstanceNorm3d(32), nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, 4, 2, 1), nn.InstanceNorm3d(64), nn.ReLU(inplace=True))\n",
    "        self.conv_mu    = nn.Conv3d(64, latent_ch, 4, 2, 1)\n",
    "        self.conv_logvar= nn.Conv3d(64, latent_ch, 4, 2, 1)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"trilinear\", align_corners=False),\n",
    "            nn.Conv3d(latent_ch, 64, 3, padding=1), nn.InstanceNorm3d(64), nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"trilinear\", align_corners=False),\n",
    "            nn.Conv3d(64, 32, 3, padding=1), nn.InstanceNorm3d(32), nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"trilinear\", align_corners=False),\n",
    "            nn.Conv3d(32, in_ch, 3, padding=1), nn.Tanh()\n",
    "        )\n",
    "        self.beta = beta\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.conv2(h)\n",
    "        mu     = self.conv_mu(h)\n",
    "        logvar = self.conv_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.dec(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        # compute KL = 0.5 * sum(μ² + σ² - logσ² - 1)\n",
    "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  \n",
    "        return recon, mu, logvar, kl\n",
    "\n",
    "\n",
    "vae = Small3DVAE(in_ch=1, latent_ch=32).to(device)\n",
    "\n",
    "# Shape & grad‐flow sanity checks\n",
    "x_test = torch.randn(2, 1, *patch_size, device=device)\n",
    "recon_test, mu_test, logvar_test, kl_test = vae(x_test)\n",
    "z_test = vae.reparameterize(mu_test, logvar_test)\n",
    "assert recon_test.shape == x_test.shape, f\"VAE recon {recon_test.shape} != {x_test.shape}\"\n",
    "assert z_test.shape == (2, 32, *(s//8 for s in patch_size)), f\"latent shape wrong: {z_test.shape}\"\n",
    "\n",
    "# 3) Train VAE\n",
    "vae_opt    = Adam(vae.parameters(), lr=1e-3)\n",
    "vae_epochs = 5\n",
    "scaler     = GradScaler()\n",
    "vae_losses = []\n",
    "\n",
    "for ep in range(vae_epochs):\n",
    "    vae.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"VAE ep{ep}\", ncols=70)\n",
    "    for batch in pbar:\n",
    "        x = batch[\"image\"].to(device)\n",
    "        vae_opt.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            recon, mu, logvar, kl = vae(x)\n",
    "            rec_loss = F.mse_loss(recon, x)\n",
    "            kl_loss  = kl / (x.numel())\n",
    "            loss     = rec_loss + vae.beta * kl_loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(vae_opt)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": total_loss/(pbar.n or 1)})\n",
    "        vae_losses.append(loss.item())\n",
    "    print(f\"→ VAE epoch {ep} avg loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# freeze VAE\n",
    "for p in vae.parameters(): p.requires_grad = False\n",
    "vae.eval()\n",
    "\n",
    "# VAE reconstruction quality on a val batch \n",
    "val_batch = next(iter(val_loader))\n",
    "x_val = val_batch[\"image\"].to(device)\n",
    "with torch.no_grad():\n",
    "    recon_val, mu_val, logvar_val, kl_val = vae(x_val)\n",
    "\n",
    "mse_val = F.mse_loss(recon_val, x_val).item()\n",
    "psnr_val = 10 * math.log10(1.0 / mse_val) if mse_val > 0 else float('inf')\n",
    "print(f\"VAE val MSE: {mse_val:.4f}, PSNR: {psnr_val:.2f} dB\")\n",
    "\n",
    "#  Build DDPM on latent space\n",
    "latent_ch   = 32\n",
    "latent_size = (s//8 for s in patch_size)\n",
    "latent_size = tuple(latent_size)\n",
    "\n",
    "ddpm = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=latent_ch,\n",
    "    out_channels=latent_ch,\n",
    "    num_channels=[32,64,128],\n",
    "    attention_levels=[False,False,False],\n",
    "    num_head_channels=[0,0,latent_ch],\n",
    "    num_res_blocks=2,\n",
    ").to(device)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"linear_beta\")\n",
    "inferer   = DiffusionInferer(scheduler=scheduler)\n",
    "ddpm_opt  = AdamW(ddpm.parameters(), lr=2e-4, weight_decay=1e-5)\n",
    "ddpm_losses = []\n",
    "\n",
    "# quick forward‐shape check\n",
    "with torch.no_grad():\n",
    "    pred_test = inferer(\n",
    "        inputs=z_test, diffusion_model=ddpm,\n",
    "        noise=torch.randn_like(z_test),\n",
    "        timesteps=torch.randint(0, scheduler.num_train_timesteps, (z_test.shape[0],), device=device)\n",
    "    )\n",
    "assert pred_test.shape == z_test.shape, f\"DDPM out {pred_test.shape} != {z_test.shape}\"\n",
    "\n",
    "# Train DDPM on latents\n",
    "n_epochs = 150\n",
    "val_interval = 25\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ddpm.train()\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"DDPM ep{epoch}\", ncols=70)\n",
    "    for batch in pbar:\n",
    "        x = batch[\"image\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = vae.encode(x)\n",
    "            z = vae.reparameterize(mu, logvar)  # B×32×8×16×16\n",
    "        ddpm_opt.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            noise = torch.randn_like(z)\n",
    "            t = torch.randint(0, scheduler.num_train_timesteps,\n",
    "                              (z.shape[0],), device=device).long()\n",
    "            pred = inferer(inputs=z, diffusion_model=ddpm,\n",
    "                           noise=noise, timesteps=t)\n",
    "            loss = F.mse_loss(pred, noise)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(ddpm_opt)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        ddpm_losses.append(loss.item())\n",
    "        pbar.set_postfix({\"loss\": epoch_loss/(pbar.n or 1)})\n",
    "    print(f\"→ DDPM epoch {epoch} avg loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        ddpm.eval()\n",
    "        # qualitative sample\n",
    "        z0 = torch.randn((1, latent_ch, *latent_size), device=device)\n",
    "        scheduler.set_timesteps(num_inference_steps=250)\n",
    "        with torch.no_grad(), autocast(device_type=\"cuda\"):\n",
    "            z_samp = inferer.sample(input_noise=z0, diffusion_model=ddpm, scheduler=scheduler)\n",
    "            out = vae.decode(z_samp).cpu()[0,0]\n",
    "        plt.figure(); plt.imshow(out[patch_size[0]//2], cmap=\"gray\"); plt.axis(\"off\")\n",
    "        plt.title(f\"Sample @ epoch {epoch}\"); plt.show()\n",
    "        \n",
    "        # diversity metric\n",
    "        zs = []\n",
    "        for _ in range(4):\n",
    "            z0 = torch.randn((1, latent_ch, *latent_size), device=device)\n",
    "            z_samp = inferer.sample(input_noise=z0, diffusion_model=ddpm, scheduler=scheduler)\n",
    "            zs.append(z_samp.flatten().cpu().numpy())\n",
    "        dists = []\n",
    "        for i in range(len(zs)):\n",
    "            for j in range(i+1, len(zs)):\n",
    "                dists.append(np.mean((zs[i]-zs[j])**2))\n",
    "        print(f\"Avg pairwise latent MSE: {np.mean(dists):.4e}\")\n",
    "\n",
    "        ddpm.train()\n",
    "\n",
    "# Final sample & display\n",
    "ddpm.eval()\n",
    "z0 = torch.randn((1, latent_ch, *latent_size), device=device)\n",
    "scheduler.set_timesteps(num_inference_steps=250)\n",
    "with torch.no_grad(), autocast(device_type=\"cuda\"):\n",
    "    z_sample   = inferer.sample(input_noise=z0, diffusion_model=ddpm, scheduler=scheduler)\n",
    "    patch_synth = vae.decode(z_sample).cpu()   # [1,1,Z,Y,X]\n",
    "\n",
    "slice_idx = patch_size[0] // 2\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(patch_synth[0, 0, slice_idx, :, :], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Final VAE+DDPM Sample\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efe5ef",
   "metadata": {},
   "source": [
    "### Visualization of the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb4abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRanged,\n",
    "    DivisiblePadd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.data import PersistentDataset, DataLoader\n",
    "\n",
    "full_transforms = Compose([\n",
    "    LoadMHAwithSimpleITK(),                         \n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    Spacingd(keys=[\"image\"], pixdim=pixdim, mode=\"linear\"),\n",
    "    DebugTap(\"after_spacing\")\n",
    "    ScaleIntensityRangePercentilesd(\n",
    "    keys=[\"image\"], lower=0.5, upper=99.5, b_min=-1, b_max=1, clip=True)\n",
    "    DivisiblePadd(\n",
    "        keys=[\"image\"], k=8,\n",
    "        mode=\"constant\", constant_values=0\n",
    "    ),                                                \n",
    "    ToTensord(keys=[\"image\"]),                      \n",
    "])\n",
    "\n",
    "viz_dataset = PersistentDataset(\n",
    "    data=data_dicts,        \n",
    "    transform=full_transforms,\n",
    "    cache_dir=\"./viz_cache\"\n",
    ")\n",
    "viz_loader = DataLoader(viz_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for ax in axes:\n",
    "    idx = random.randrange(len(viz_dataset))\n",
    "    vol = viz_dataset[idx][\"image\"][0].cpu().numpy()  # [Z, Y, X]\n",
    "    mid = vol.shape[0] // 2\n",
    "    ax.imshow(vol[mid], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax.set_title(f\"Volume {idx}, slice {mid}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22296e5",
   "metadata": {},
   "source": [
    "### Define network, scheduler, optimizer, and inferer\n",
    "\n",
    "We will use a DDPM in this example; for that, we need to define a `DiffusionModelUNet` network that will have as input the noisy images and the values for the timestep `t`, and it will predict the noise that is present in the image.\n",
    "\n",
    "In this example, we have a network with three levels (with 256, 256, and 512 channels in each). In every level, we will have two residual blocks, and only the last one will have an attention block with a single attention head (with 512 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499f7b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1, out_channels=1,\n",
    "    num_channels=[32, 64, 128],      # 3 levels\n",
    "    attention_levels=[False, False, False],\n",
    "    num_head_channels=[0, 64, 128],\n",
    "    num_res_blocks=2,\n",
    ").to(device)\n",
    "\n",
    "DivisiblePadd(keys=[\"image\"], k=16, mode=\"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad91ff",
   "metadata": {},
   "source": [
    "Together with our U-net, we need to define the Noise Scheduler for the diffusion model. This scheduler is responsible for defining the amount of noise that should be added in each timestep `t` of the diffusion model's Markov chain. Besides that, it has the operations to perform the reverse process, which will remove the noise of the images (a.k.a. denoising process). In this case, we are using a `DDPMScheduler`. Here we are using 1000 timesteps and a `scaled_linear` profile for the beta values (proposed in [Rombach et al. \"High-Resolution Image Synthesis with Latent Diffusion Models\"](https://arxiv.org/abs/2112.10752)). This profile had better results than the `linear, proposed in the original DDPM's paper. In `beta_start` and `beta_end`, we define the limits for the beta values. These are important to determine how accentuated is the addition of noise in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1de5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scheduler.alphas_cumprod.cpu(), color=(2 / 255, 163 / 255, 163 / 255), linewidth=2)\n",
    "plt.xlabel(\"Timestep [t]\")\n",
    "plt.ylabel(\"alpha cumprod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125f7c8",
   "metadata": {},
   "source": [
    "Finally, we define the Inferer, which contains functions that will help during the training and sampling of the model, and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f371ad8",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "In this part, we will train the diffusion model to predict the noise added to the images. For this, we are using an MSE loss between the prediction and the original noise. During the training, we are also sampling brain images to evaluate the evolution of the model. In this training, we use Automatic Mixed Precision to save memory and speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10b595",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "val_interval = 25\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "\n",
    "            # Create timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "            ).long()\n",
    "\n",
    "            # Get model prediction\n",
    "            noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            with torch.no_grad():\n",
    "                with autocast(device_type=\"cuda\"):\n",
    "                    timesteps = torch.randint(\n",
    "                        0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                    ).long()\n",
    "\n",
    "                    # Get model prediction\n",
    "                    noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                    val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            progress_bar.set_postfix({\"val_loss\": val_epoch_loss / (step + 1)})\n",
    "        val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "\n",
    "        # Sampling image during training\n",
    "        image = torch.randn((1, 1, 32, 40, 32))\n",
    "        image = image.to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            image = inferer.sample(input_noise=image, diffusion_model=model, scheduler=scheduler)\n",
    "\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(image[0, 0, :, :, 15].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")\n",
    "\n",
    "vae.eval()\n",
    "x = train_ds[0][\"image\"].unsqueeze(0).to(device)   # original patch\n",
    "with torch.no_grad():\n",
    "    recon, _ = vae(x)\n",
    "\n",
    "def show_slice(vol, title=\"\", rescale=True):\n",
    "    \"\"\"\n",
    "    Display the middle axial slice of a (Z,Y,X) volume.\n",
    "    If `rescale=True` the data are mapped to [0,1] just for plotting.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(vol):\n",
    "        vol = vol.detach().cpu()\n",
    "    vol = vol.numpy()           \n",
    "\n",
    "    z_mid      = vol.shape[0] // 2\n",
    "    slice_img  = vol[z_mid]      \n",
    "\n",
    "    if rescale:                 \n",
    "        slice_img = torch.tanh(torch.as_tensor(slice_img))\n",
    "        slice_img = (slice_img + 1) / 2                     \n",
    "        slice_img = slice_img.numpy()\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(slice_img, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(title); plt.axis(\"off\"); plt.show()\n",
    "vol = recon[0, 0]    # now vol.shape == (Z, Y, X)\n",
    "show_slice(x[0,0], \"Input patch\")\n",
    "show_slice(vol, \"VAE recon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e263b67",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7520419",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
    "    val_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38724c9b",
   "metadata": {},
   "source": [
    "## Sampling Brain Image\n",
    "\n",
    "In order to sample the brain images, we need to pass the model an image containing just noise and use it to remove the noise of the image iteratively. For that, we will use the `.sample()` function of the `inferer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092eb6a0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ddpm.eval()\n",
    "vae.eval()\n",
    "\n",
    "latent_noise = torch.randn((1, 32, 8, 16, 16), device=device)\n",
    "scheduler.set_timesteps(num_inference_steps=250)\n",
    "\n",
    "with torch.no_grad(), autocast(device_type=\"cuda\"):\n",
    "    latent_denoised = inferer.sample(\n",
    "        input_noise      = latent_noise,\n",
    "        diffusion_model  = ddpm,\n",
    "        scheduler        = scheduler\n",
    "    )\n",
    "\n",
    "    patch = vae.decode(latent_denoised)  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "mid_z = patch.shape[2] // 2\n",
    "plt.imshow(patch[0, 0, mid_z].cpu(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Synthetic patch (slice {mid_z})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acc27a",
   "metadata": {},
   "source": [
    "### Sampling with Denoising Diffusion Implicit Model Scheduler\n",
    "\n",
    "Recent papers have proposed different ways to improve the sampling speed by using fewer steps in the denoising process. In this example, we are using a `DDIMScheduler` (from [Song et al. \"Denoising Diffusion Implicit Models\"](https://arxiv.org/abs/2010.02502)) to reduce the original number of steps from 1000 to 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e43b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative.networks.schedulers import DDIMScheduler\n",
    "\n",
    "scheduler_ddim = DDIMScheduler(\n",
    "    num_train_timesteps = 1000,\n",
    "    schedule            = \"scaled_linear_beta\",\n",
    "    beta_start          = 0.0005,\n",
    "    beta_end            = 0.0195,\n",
    "    clip_sample         = False,                  \n",
    ")\n",
    "\n",
    "scheduler_ddim.set_timesteps(num_inference_steps = 250)\n",
    "ddpm.eval()\n",
    "vae.eval()\n",
    "latent_noise = torch.randn((1, 32, 8, 16, 16), device=device)\n",
    "\n",
    "with torch.no_grad(), autocast(device_type=\"cuda\"):\n",
    "    latent_denoised = inferer.sample(\n",
    "        input_noise     = latent_noise,\n",
    "        diffusion_model = ddpm,\n",
    "        scheduler       = scheduler_ddim,\n",
    "    )\n",
    "    patch = vae.decode(latent_denoised\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "mid_z = patch.shape[2] // 2\n",
    "plt.imshow(patch[0, 0, mid_z].cpu(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"DDIM sample (slice {mid_z})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73a9bf-4ba1-4029-8228-f26b5d4d9f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
